{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Optional: If you want to replace TA-Lib with pandas_ta\n",
    "# import pandas_ta as ta"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set plot style\n",
    "sns.set(style='whitegrid')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define base directory (assumes your notebook is inside 'notebooks' folder)\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Paths to your data (relative to project root)\n",
    "NEWS_DATA_PATH = os.path.join(base_dir, 'Data', 'Data', 'raw_analyst_ratings', 'raw_analyst_ratings.csv')\n",
    "TSLA_PATH = os.path.join(base_dir, 'Data', 'Data', 'yfinance_data', 'yfinance_data', 'TSLA_historical_data.csv')\n",
    "NVDA_PATH = os.path.join(base_dir, 'Data', 'Data', 'yfinance_data', 'yfinance_data', 'NVDA_historical_data.csv')\n",
    "MSFT_PATH = os.path.join(base_dir, 'Data', 'Data', 'yfinance_data', 'yfinance_data', 'MSFT_historical_data.csv')\n",
    "META_PATH = os.path.join(base_dir, 'Data', 'Data', 'yfinance_data', 'yfinance_data', 'META_historical_data.csv')\n",
    "GOOG_PATH = os.path.join(base_dir, 'Data', 'Data', 'yfinance_data', 'yfinance_data', 'GOOG_historical_data.csv')\n",
    "AMZN_PATH = os.path.join(base_dir, 'Data', 'Data', 'yfinance_data', 'yfinance_data', 'AMZN_historical_data.csv')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check file existence\n",
    "paths = {\n",
    "    'News Data': NEWS_DATA_PATH,\n",
    "    'TSLA': TSLA_PATH,\n",
    "    'NVDA': NVDA_PATH,\n",
    "    'MSFT': MSFT_PATH,\n",
    "    'META': META_PATH,\n",
    "    'GOOG': GOOG_PATH,\n",
    "    'AMZN': AMZN_PATH\n",
    "}\n",
    "for name, path in paths.items():\n",
    "    print(f\"{name} exists: {os.path.exists(path)}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load news data\n",
    "# Load without parse_dates, then convert explicitly\n",
    "news_df = pd.read_csv(NEWS_DATA_PATH)\n",
    "\n",
    "# Convert 'date' column to datetime explicitly\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid 'date' if any\n",
    "news_df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Create 'publication_date' from 'date'\n",
    "news_df['publication_date'] = news_df['date'].dt.date"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load stock data\n",
    "tsla_df = pd.read_csv(TSLA_PATH, parse_dates=['Date'])\n",
    "nvda_df = pd.read_csv(NVDA_PATH, parse_dates=['Date'])\n",
    "msft_df = pd.read_csv(MSFT_PATH, parse_dates=['Date'])\n",
    "meta_df = pd.read_csv(META_PATH, parse_dates=['Date'])\n",
    "goog_df = pd.read_csv(GOOG_PATH, parse_dates=['Date'])\n",
    "amzn_df = pd.read_csv(AMZN_PATH, parse_dates=['Date'])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set Date as index for stock data\n",
    "tsla_df.set_index('Date', inplace=True)\n",
    "nvda_df.set_index('Date', inplace=True)\n",
    "msft_df.set_index('Date', inplace=True)\n",
    "meta_df.set_index('Date', inplace=True)\n",
    "goog_df.set_index('Date', inplace=True)\n",
    "amzn_df.set_index('Date', inplace=True)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Basic statistics for headline length\n",
    "news_df['headline_length'] = news_df['headline'].astype(str).apply(len)\n",
    "print(\"Headline Length Descriptive Statistics:\")\n",
    "print(news_df['headline_length'].describe())"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. Count articles per publisher\n",
    "publisher_counts = news_df['publisher'].value_counts()\n",
    "print(\"\\nTop 10 Publishers by Number of Articles:\")\n",
    "print(publisher_counts.head(10))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Publication date trend over time\n",
    "pub_date_counts = news_df['publication_date'].value_counts().sort_index()\n",
    "plt.figure(figsize=(12,6))\n",
    "pub_date_counts.plot()\n",
    "plt.title('Number of Articles Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Analysis (Topic Modeling)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Fill NaNs\n",
    "texts = news_df['headline'].fillna('')\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "dtm = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Apply LDA Topic Modeling\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Display top words per topic\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f\"\\nTopic {index + 1}:\")\n",
    "    top_indices = topic.argsort()[-10:][::-1]\n",
    "    top_words = [vectorizer.get_feature_names_out()[i] for i in top_indices]\n",
    "    print(top_words)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time Series Analysis of News Publishing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Using 'publication_date' for time series analysis\n",
    "pub_counts_over_time = news_df['publication_date'].value_counts().sort_index()\n",
    "plt.figure(figsize=(12,6))\n",
    "pub_counts_over_time.plot()\n",
    "plt.title('News Articles Frequency Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. Publishing times analysis\n",
    "if 'publication_time' in news_df.columns:\n",
    "    news_df['pub_hour'] = pd.to_datetime(news_df['publication_time']).dt.hour\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.countplot(x='pub_hour', data=news_df)\n",
    "    plt.title('Articles Published by Hour of Day')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. Publisher contribution analysis\n",
    "top_publishers = news_df['publisher'].value_counts().head(10)\n",
    "print(\"\\nTop 10 Publishers:\")\n",
    "print(top_publishers)\n",
    "\n",
    "# Extract domains if publisher looks like email\n",
    "def extract_domain(publisher_name):\n",
    "    match = re.search(r'@([\\w.-]+)', str(publisher_name))\n",
    "    return match.group(1) if match else publisher_name\n",
    "\n",
    "news_df['domain'] = news_df['publisher'].apply(extract_domain)\n",
    "domain_counts = news_df['domain'].value_counts()\n",
    "print(\"\\nTop Domains contributing to news:\")\n",
    "print(domain_counts.head(10))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stock Data Analysis (Using pandas and pandas_ta if installed)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Example for Tesla\n",
    "# Uncomment if pandas_ta is installed\n",
    "# import pandas_ta as ta\n",
    "# tsla_df['SMA_20'] = tsla_df['Close'].ta.sma(length=20)\n",
    "# tsla_df['RSI'] = tsla_df['Close'].ta.rsi()\n",
    "# macd = tsla_df['Close'].ta.macd()\n",
    "# tsla_df['MACD'] = macd['MACD']\n",
    "# tsla_df['MACD_signal'] = macd['MACDs']\n",
    "# tsla_df['MACD_hist'] = macd['MACDh']\n",
    "\n",
    "# Plot Close and SMA (if computed)\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(tsla_df['Close'], label='Close Price')\n",
    "# plt.plot(tsla_df['SMA_20'], label='20-day SMA')\n",
    "plt.title('Tesla Stock Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6. Additional Financial Metrics\n",
    "tsla_df['Daily_Return'] = tsla_df['Close'].pct_change()\n",
    "print(\"\\nTesla Daily Returns Summary:\")\n",
    "print(tsla_df['Daily_Return'].describe())"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Analysis complete.**"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
